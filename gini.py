# -*- coding: utf-8 -*-
"""Untitled48.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tbX3B-oEPV3tHaL0MNwJgDMn4sjauhNa
"""

import os
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import pandas as pd
import networkx as nx
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score
from torch_geometric.nn import GCNConv

# -------------------- CONFIG --------------------

DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'
torch.manual_seed(42)
np.random.seed(42)

ROOT = "/Users/abhayatrivedi/Documents/projects/twinmark"
BELIEF_PATH = "belief_1000.csv"

EMBED_DIM = 128
NUM_CLASSES = 3
ATTACK_RATIO = 0.15
ATTACK_STRENGTH = 3.0
EPOCHS = 200
LR = 0.005


belief_map = {
    "非常悲观": -1.0,
    "悲观的": -0.6,
    "中性的": 0.0,
    "乐观的": 0.6,
    "非常乐观": 1.0
}

def encode_belief(val):
    if isinstance(val, str):
        return belief_map.get(val.strip(), 0.0)
    return float(val)


print(f"[INFO] Loading TwinMarket belief log: {BELIEF_PATH}")
belief_df = pd.read_csv(BELIEF_PATH)
print("Columns:", belief_df.columns)

# -------------------- BELIEF GRAPH --------------------

def build_belief_graph(df):
    G = nx.Graph()
    for _, row in df.iterrows():
        agent = f"A_{row.iloc[0]}"
        asset = f"S_{row.iloc[1]}"
        belief = encode_belief(row.iloc[2])
        G.add_edge(agent, asset, weight=belief)
    return G

belief_graph = build_belief_graph(belief_df)
node_map = {n: i for i, n in enumerate(belief_graph.nodes())}

edges = []
for u, v in belief_graph.edges():
    edges.append([node_map[u], node_map[v]])
    edges.append([node_map[v], node_map[u]])

edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous().to(DEVICE)

# -------------------- NODE FEATURES --------------------

def belief_to_features(df, node_map, dim):
    X = torch.zeros(len(node_map), dim)
    for _, row in df.iterrows():
        agent = f"A_{row.iloc[0]}"
        idx = node_map[agent]
        belief = encode_belief(row.iloc[2])
        X[idx] += belief * torch.randn(dim)
    return X

X = belief_to_features(belief_df, node_map, EMBED_DIM).to(DEVICE)
y = torch.randint(0, NUM_CLASSES, (X.shape[0],), device=DEVICE)

# -------------------- MODELS --------------------

class GNNTeacher(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = GCNConv(EMBED_DIM, 256)
        self.conv2 = GCNConv(256, NUM_CLASSES)

    def forward(self, x, edge_index):
        h = F.relu(self.conv1(x, edge_index))
        return self.conv2(h, edge_index)

class MLPStudent(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(EMBED_DIM, 256)
        self.fc2 = nn.Linear(256, NUM_CLASSES)

    def forward(self, x):
        return self.fc2(F.relu(self.fc1(x)))

# -------------------- ATTACK --------------------

def inject_belief_attack(X, ratio=0.15, strength=3.0):
    n = X.shape[0]
    k = int(ratio * n)
    idx = torch.randperm(n)[:k]
    X_adv = X.clone()
    X_adv[idx] += strength * torch.randn_like(X_adv[idx])
    return X_adv

# -------------------- DISTILLATION --------------------

def distill(teacher, student, X, edge_index, y):
    opt = torch.optim.Adam(student.parameters(), lr=LR)
    for epoch in range(EPOCHS):
        teacher.eval()
        student.train()

        with torch.no_grad():
            t_logits = teacher(X, edge_index)

        s_logits = student(X)

        loss_kd = F.kl_div(
            F.log_softmax(s_logits, dim=1),
            F.softmax(t_logits, dim=1),
            reduction='batchmean'
        )
        loss_sup = F.cross_entropy(s_logits, y)
        loss = 0.7 * loss_kd + 0.3 * loss_sup

        opt.zero_grad()
        loss.backward()
        opt.step()

        if epoch % 50 == 0:
            print(f"[KD] Epoch {epoch:03d} | Loss {loss.item():.4f}")

# -------------------- PRICE SIMULATION --------------------

def simulate_price(logits):
    probs = F.softmax(logits, dim=1)
    return (probs[:, 1] - probs[:, 0]).detach().cpu().numpy()

# -------------------- EXPERIMENT --------------------

teacher = GNNTeacher().to(DEVICE)
opt = torch.optim.Adam(teacher.parameters(), lr=LR)

print("Training GNN Teacher...")

for epoch in range(EPOCHS):
    teacher.train()
    out = teacher(X, edge_index)
    loss = F.cross_entropy(out, y)

    opt.zero_grad()
    loss.backward()
    opt.step()

    if epoch % 50 == 0:
        print(f"[GNN] Epoch {epoch:03d} | Loss {loss.item():.4f}")

student = MLPStudent().to(DEVICE)

print("Distilling → MLP Student")
distill(teacher, student, X, edge_index, y)

print("Injecting belief misinformation attack")
X_adv = inject_belief_attack(X, ATTACK_RATIO, ATTACK_STRENGTH)

teacher.eval()
student.eval()

with torch.no_grad():
    gnn_clean = teacher(X, edge_index)
    gnn_adv   = teacher(X_adv, edge_index)
    mlp_clean = student(X)
    mlp_adv   = student(X_adv)

p_gnn_clean = simulate_price(gnn_clean)
p_gnn_adv   = simulate_price(gnn_adv)
p_mlp_clean = simulate_price(mlp_clean)
p_mlp_adv   = simulate_price(mlp_adv)

# -------------------- RESULTS --------------------

print("SYSTEMIC FRAGILITY RESULTS")
print(f"GNN Volatility: {np.std(p_gnn_adv - p_gnn_clean):.4f}")
print(f"MLP Volatility: {np.std(p_mlp_adv - p_mlp_clean):.4f}")

plt.figure(figsize=(11,4))
plt.plot(p_gnn_adv - p_gnn_clean, label="GNN Shock", alpha=0.9)
plt.plot(p_mlp_adv - p_mlp_clean, label="MLP Shock", alpha=0.9)
plt.legend()
plt.title("Market Instability under Adversarial Belief Manipulation")
plt.xlabel("Market Agent")
plt.ylabel("Price Shock")
plt.grid(alpha=0.3)
plt.show()

# ============================================================
# TwinMarket GNN vs MLP — Experiments 1,2,3
# ============================================================

import os
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import pandas as pd
import networkx as nx
import matplotlib.pyplot as plt
from torch_geometric.nn import GCNConv
import yfinance as yf

DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'
torch.manual_seed(42)
np.random.seed(42)

# ============================================================
# LOAD REAL MARKET SHOCKS
# ============================================================

sp = yf.download("^GSPC", start="2007-01-01", end="2024-01-01", progress=False)
returns = np.log(sp['Close']).diff().dropna().values.flatten()
print("[INFO] Loaded S&P500 shocks:", returns.shape)

# ============================================================
# LOAD BELIEF GRAPH (TwinMarket)
# ============================================================

ROOT = "/Users/abhayatrivedi/Documents/projects/twinmark"
BELIEF_PATH = "belief_1000.csv"

belief_df = pd.read_csv(BELIEF_PATH)

belief_map = {
    "非常悲观": -1.0,
    "悲观的": -0.6,
    "中性的": 0.0,
    "乐观的": 0.6,
    "非常乐观": 1.0
}

def encode_belief(v):
    return belief_map.get(v.strip(), 0.0)

# Build belief graph
G = nx.Graph()
for _, r in belief_df.iterrows():
    a = f"A_{r.iloc[0]}"
    s = f"S_{r.iloc[1]}"
    G.add_edge(a, s)

node_map = {n: i for i,n in enumerate(G.nodes())}
edges = []
for u,v in G.edges():
    edges.append([node_map[u], node_map[v]])
    edges.append([node_map[v], node_map[u]])

edge_index = torch.tensor(edges).t().to(DEVICE)

# Node features
EMBED = 128
X = torch.zeros(len(node_map), EMBED)
for _, r in belief_df.iterrows():
    idx = node_map[f"A_{r.iloc[0]}"]
    X[idx] += encode_belief(r.iloc[2]) * torch.randn(EMBED)

X = X.to(DEVICE)
y = torch.randint(0, 3, (X.shape[0],), device=DEVICE)

# ============================================================
# MODELS (UNCHANGED)
# ============================================================

class GNNTeacher(nn.Module):
    def __init__(self):
        super().__init__()
        self.c1 = GCNConv(EMBED, 256)
        self.c2 = GCNConv(256, 3)
    def forward(self, x, ei):
        return self.c2(F.relu(self.c1(x, ei)), ei)

class MLPStudent(nn.Module):
    def __init__(self):
        super().__init__()
        self.f1 = nn.Linear(EMBED, 256)
        self.f2 = nn.Linear(256, 3)
    def forward(self, x):
        return self.f2(F.relu(self.f1(x)))

# ============================================================
# TRAIN TEACHER + DISTILL STUDENT (UNCHANGED)
# ============================================================

teacher = GNNTeacher().to(DEVICE)
opt = torch.optim.Adam(teacher.parameters(), lr=0.005)

for _ in range(200):
    out = teacher(X, edge_index)
    loss = F.cross_entropy(out, y)
    opt.zero_grad(); loss.backward(); opt.step()

student = MLPStudent().to(DEVICE)
opt = torch.optim.Adam(student.parameters(), lr=0.005)

for _ in range(200):
    with torch.no_grad():
        tlog = teacher(X, edge_index)
    slog = student(X)
    loss = F.kl_div(
        F.log_softmax(slog/6,1),
        F.softmax(tlog/6,1),
        reduction='batchmean'
    )
    opt.zero_grad(); loss.backward(); opt.step()

# ============================================================
# ATTACK (ONLY THING THAT CHANGES)
# ============================================================

def inject_attack(X, edge_index, returns, ratio=0.15):
    X_adv = X.clone()
    n = X.shape[0]
    k = int(ratio * n)

    shock = abs(returns[np.random.randint(len(returns))])
    strength = 5 + 15 * shock

    deg = torch.bincount(edge_index[0], minlength=n).float()
    hubs = torch.topk(deg, k).indices

    direction = torch.sign(torch.randn(1, device=DEVICE))
    X_adv[hubs] += direction * strength

    nbrs = edge_index[1][torch.isin(edge_index[0], hubs)]
    X_adv[nbrs] += 0.6 * direction * strength

    return X_adv

# ============================================================
# PRICE PROXY
# ============================================================

def price(logits):
    p = F.softmax(logits,1)
    return (p[:,1] - p[:,0]).cpu().numpy()

# ============================================================
# EXPERIMENT 1 — WEALTH INEQUALITY
# ============================================================

def exp1(model, is_gnn):
    X_adv = inject_attack(X, edge_index, returns)
    with torch.no_grad():
        log = model(X_adv, edge_index) if is_gnn else model(X_adv)
    wealth = np.cumsum(price(log))
    gini = np.abs(np.subtract.outer(wealth,wealth)).mean() / (2*np.mean(np.abs(wealth)))
    return gini

# ============================================================
# EXPERIMENT 2 — TRADING ACTIVITY vs RETURNS
# ============================================================

def exp2(model, is_gnn):
    X_adv = inject_attack(X, edge_index, returns)
    with torch.no_grad():
        c = model(X, edge_index) if is_gnn else model(X)
        a = model(X_adv, edge_index) if is_gnn else model(X_adv)
    delta = price(a) - price(c)
    volume = np.abs(delta)
    return np.corrcoef(volume, delta)[0,1]

# ============================================================
# EXPERIMENT 3 — FAT TAILS (SF I)
# ============================================================

def exp3(model, is_gnn):
    with torch.no_grad():
        log = model(X, edge_index) if is_gnn else model(X)
    r = price(log)
    return np.mean(np.abs(r) > 2*np.std(r))

# ============================================================
# RUN ALL
# ============================================================

TRIALS = 20

print("\n==== FINAL RESULTS ====\n")

print("EXP 1 — Gini")
print("GNN:", np.mean([exp1(teacher,True) for _ in range(TRIALS)]))
print("MLP:", np.mean([exp1(student,False) for _ in range(TRIALS)]))

print("\nEXP 2 — Volume–Return Corr")
print("GNN:", np.mean([exp2(teacher,True) for _ in range(TRIALS)]))
print("MLP:", np.mean([exp2(student,False) for _ in range(TRIALS)]))

print("\nEXP 3 — Fat Tails")
print("GNN:", np.mean([exp3(teacher,True) for _ in range(TRIALS)]))
print("MLP:", np.mean([exp3(student,False) for _ in range(TRIALS)]))



import os
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import pandas as pd
import networkx as nx
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score
from torch_geometric.nn import GCNConv

# -------------------- CONFIG --------------------

DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'
torch.manual_seed(42)
np.random.seed(42)

ROOT = "/Users/abhayatrivedi/Documents/projects/twinmark"
BELIEF_PATH = "belief_1000.csv"

EMBED_DIM = 128
NUM_CLASSES = 3
ATTACK_RATIO = 0.15
ATTACK_STRENGTH = 3.0
EPOCHS = 200
LR = 0.005


belief_map = {
    "非常悲观": -1.0,
    "悲观的": -0.6,
    "中性的": 0.0,
    "乐观的": 0.6,
    "非常乐观": 1.0
}

def encode_belief(val):
    if isinstance(val, str):
        return belief_map.get(val.strip(), 0.0)
    return float(val)


print(f"[INFO] Loading TwinMarket belief log: {BELIEF_PATH}")
belief_df = pd.read_csv(BELIEF_PATH)
print("Columns:", belief_df.columns)

# -------------------- BELIEF GRAPH --------------------

def build_belief_graph(df):
    G = nx.Graph()
    for _, row in df.iterrows():
        agent = f"A_{row.iloc[0]}"
        asset = f"S_{row.iloc[1]}"
        belief = encode_belief(row.iloc[2])
        G.add_edge(agent, asset, weight=belief)
    return G

belief_graph = build_belief_graph(belief_df)
node_map = {n: i for i, n in enumerate(belief_graph.nodes())}

edges = []
for u, v in belief_graph.edges():
    edges.append([node_map[u], node_map[v]])
    edges.append([node_map[v], node_map[u]])

edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous().to(DEVICE)

# -------------------- NODE FEATURES --------------------

def belief_to_features(df, node_map, dim):
    X = torch.zeros(len(node_map), dim)
    for _, row in df.iterrows():
        agent = f"A_{row.iloc[0]}"
        idx = node_map[agent]
        belief = encode_belief(row.iloc[2])
        X[idx] += belief * torch.randn(dim)
    return X

X = belief_to_features(belief_df, node_map, EMBED_DIM).to(DEVICE)
y = torch.randint(0, NUM_CLASSES, (X.shape[0],), device=DEVICE)

# -------------------- MODELS --------------------

class GNNTeacher(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = GCNConv(EMBED_DIM, 256)
        self.conv2 = GCNConv(256, NUM_CLASSES)

    def forward(self, x, edge_index):
        h = F.relu(self.conv1(x, edge_index))
        return self.conv2(h, edge_index)

class MLPStudent(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(EMBED_DIM, 256)
        self.fc2 = nn.Linear(256, NUM_CLASSES)

    def forward(self, x):
        return self.fc2(F.relu(self.fc1(x)))

# -------------------- ATTACK --------------------

def inject_belief_attack(X, ratio=0.15, strength=3.0):
    n = X.shape[0]
    k = int(ratio * n)
    idx = torch.randperm(n)[:k]
    X_adv = X.clone()
    X_adv[idx] += strength * torch.randn_like(X_adv[idx])
    return X_adv

# -------------------- DISTILLATION --------------------

def distill(teacher, student, X, edge_index, y):
    opt = torch.optim.Adam(student.parameters(), lr=LR)
    for epoch in range(EPOCHS):
        teacher.eval()
        student.train()

        with torch.no_grad():
            t_logits = teacher(X, edge_index)

        s_logits = student(X)

        loss_kd = F.kl_div(
            F.log_softmax(s_logits, dim=1),
            F.softmax(t_logits, dim=1),
            reduction='batchmean'
        )
        loss_sup = F.cross_entropy(s_logits, y)
        loss = 0.7 * loss_kd + 0.3 * loss_sup

        opt.zero_grad()
        loss.backward()
        opt.step()

        if epoch % 50 == 0:
            print(f"[KD] Epoch {epoch:03d} | Loss {loss.item():.4f}")

# -------------------- PRICE SIMULATION --------------------

def simulate_price(logits):
    probs = F.softmax(logits, dim=1)
    return (probs[:, 1] - probs[:, 0]).detach().cpu().numpy()

# -------------------- EXPERIMENT --------------------

teacher = GNNTeacher().to(DEVICE)
opt = torch.optim.Adam(teacher.parameters(), lr=LR)

print("Training GNN Teacher...")

for epoch in range(EPOCHS):
    teacher.train()
    out = teacher(X, edge_index)
    loss = F.cross_entropy(out, y)

    opt.zero_grad()
    loss.backward()
    opt.step()

    if epoch % 50 == 0:
        print(f"[GNN] Epoch {epoch:03d} | Loss {loss.item():.4f}")

student = MLPStudent().to(DEVICE)

print("Distilling → MLP Student")
distill(teacher, student, X, edge_index, y)

print("Injecting belief misinformation attack")
X_adv = inject_belief_attack(X, ATTACK_RATIO, ATTACK_STRENGTH)

teacher.eval()
student.eval()

with torch.no_grad():
    gnn_clean = teacher(X, edge_index)
    gnn_adv   = teacher(X_adv, edge_index)
    mlp_clean = student(X)
    mlp_adv   = student(X_adv)

p_gnn_clean = simulate_price(gnn_clean)
p_gnn_adv   = simulate_price(gnn_adv)
p_mlp_clean = simulate_price(mlp_clean)
p_mlp_adv   = simulate_price(mlp_adv)

# -------------------- RESULTS --------------------

print("SYSTEMIC FRAGILITY RESULTS")
print(f"GNN Volatility: {np.std(p_gnn_adv - p_gnn_clean):.4f}")
print(f"MLP Volatility: {np.std(p_mlp_adv - p_mlp_clean):.4f}")

plt.figure(figsize=(11,4))
plt.plot(p_gnn_adv - p_gnn_clean, label="GNN Shock", alpha=0.9)
plt.plot(p_mlp_adv - p_mlp_clean, label="MLP Shock", alpha=0.9)
plt.legend()
plt.title("Market Instability under Adversarial Belief Manipulation")
plt.xlabel("Market Agent")
plt.ylabel("Price Shock")
plt.grid(alpha=0.3)
plt.show()

# ============================================================
# GNN vs MLP — Belief Attack Experiments (1,2,3)
# ============================================================

import numpy as np
import torch
import torch.nn.functional as F

# -------------------------
# Shared utilities
# -------------------------

def price_proxy(logits):
    """
    Maps logits → scalar market signal
    """
    probs = F.softmax(logits, dim=1)
    return (probs[:,1] - probs[:,0]).detach().cpu().numpy()


def inject_belief_attack(X, edge_index, returns, ratio=0.15):
    """
    ONLY intervention knob.
    Identical X_adv is fed to GNN & MLP.
    """
    X_adv = X.clone()
    n = X.shape[0]
    k = int(ratio * n)

    t = np.random.randint(len(returns))
    shock = abs(returns[t])
    strength = 4.0 + 15.0 * shock

    deg = torch.bincount(edge_index[0], minlength=n).float()
    hubs = torch.topk(deg, k).indices

    direction = torch.sign(torch.randn(1, device=X.device))

    # hub corruption
    X_adv[hubs] += direction * strength

    # cascade
    nbrs = edge_index[1][torch.isin(edge_index[0], hubs)]
    X_adv[nbrs] += 0.6 * direction * strength

    return X_adv


# -------------------------
# EXPERIMENT 1
# Wealth Inequality Sensitivity
# -------------------------

def experiment_1_wealth(model, is_gnn):
    X_adv = inject_belief_attack(X, edge_index, returns)

    with torch.no_grad():
        logits = model(X_adv, edge_index) if is_gnn else model(X_adv)

    wealth = np.cumsum(price_proxy(logits))

    gini = np.abs(np.subtract.outer(wealth, wealth)).mean() / \
           (2 * np.mean(np.abs(wealth)) + 1e-6)

    return gini


# -------------------------
# EXPERIMENT 2
# Trading Activity vs Returns
# -------------------------

def experiment_2_turnover(model, is_gnn):
    X_adv = inject_belief_attack(X, edge_index, returns)

    with torch.no_grad():
        clean = model(X, edge_index) if is_gnn else model(X)
        adv   = model(X_adv, edge_index) if is_gnn else model(X_adv)

    delta = price_proxy(adv) - price_proxy(clean)
    volume = np.abs(delta)

    return np.corrcoef(volume, delta)[0,1]


# -------------------------
# EXPERIMENT 3
# Fat Tails (Stylized Fact I)
# -------------------------

def experiment_3_fat_tails(model, is_gnn):
    with torch.no_grad():
        logits = model(X, edge_index) if is_gnn else model(X)

    r = price_proxy(logits)
    return np.mean(np.abs(r) > 2 * np.std(r))


# -------------------------
# RUN ALL EXPERIMENTS
# -------------------------

TRIALS = 20

print("\n================= GNN vs MLP RESULTS =================\n")

# ---- EXP 1 ----
g1 = np.mean([experiment_1_wealth(teacher, True) for _ in range(TRIALS)])
m1 = np.mean([experiment_1_wealth(student, False) for _ in range(TRIALS)])

print("EXP-1 | Wealth Inequality (Gini)")
print(f"GNN  : {g1:.4f}")
print(f"MLP  : {m1:.4f}\n")

# ---- EXP 2 ----
g2 = np.mean([experiment_2_turnover(teacher, True) for _ in range(TRIALS)])
m2 = np.mean([experiment_2_turnover(student, False) for _ in range(TRIALS)])

print("EXP-2 | Trading Activity vs Returns")
print(f"GNN  : {g2:.4f}")
print(f"MLP  : {m2:.4f}\n")

# ---- EXP 3 ----
g3 = np.mean([experiment_3_fat_tails(teacher, True) for _ in range(TRIALS)])
m3 = np.mean([experiment_3_fat_tails(student, False) for _ in range(TRIALS)])

print("EXP-3 | Fat Tails (SF-I)")
print(f"GNN  : {g3:.4f}")
print(f"MLP  : {m3:.4f}")

print("\n======================================================")



import matplotlib.pyplot as plt
import numpy as np

TRIALS = 50  # or whatever number you have

# Collect wealth values for each trial
gnn_wealth = [experiment_1_wealth(teacher, True) for _ in range(TRIALS)]
mlp_wealth = [experiment_1_wealth(student, False) for _ in range(TRIALS)]

# Compute means
g1 = np.mean(gnn_wealth)
m1 = np.mean(mlp_wealth)

print("EXP-1 | Wealth Inequality (Gini)")
print(f"GNN  : {g1:.4f}")
print(f"MLP  : {m1:.4f}\n")

# Plot the distribution over trials
plt.figure(figsize=(10, 5))
plt.plot(range(1, TRIALS+1), gnn_wealth, label='GNN', marker='o')
plt.plot(range(1, TRIALS+1), mlp_wealth, label='MLP', marker='x')
plt.axhline(g1, color='blue', linestyle='--', label='GNN Mean')
plt.axhline(m1, color='orange', linestyle='--', label='MLP Mean')
plt.xlabel('Trial')
plt.ylabel('Wealth (Gini)')
plt.title('Wealth Inequality over Trials')
plt.legend()
plt.grid(True)
plt.show()

"""“While both models generate substantial wealth inequality, MLP-driven markets exhibit marginally higher Gini coefficients due to unstructured, agent-level volatility, whereas GNN-induced inequality arises from coordinated belief propagation.”"""

import matplotlib.pyplot as plt
import numpy as np

TRIALS = 50  # number of trials

# ---- EXP 2 ----
gnn_exp2 = [experiment_2_turnover(teacher, True) for _ in range(TRIALS)]
mlp_exp2 = [experiment_2_turnover(student, False) for _ in range(TRIALS)]

g2 = np.mean(gnn_exp2)
m2 = np.mean(mlp_exp2)

print("EXP-2 | Trading Activity vs Returns")
print(f"GNN  : {g2:.4f}")
print(f"MLP  : {m2:.4f}\n")

# Plot EXP-2
plt.figure(figsize=(10,5))
plt.plot(range(1, TRIALS+1), gnn_exp2, label='GNN', marker='o')
plt.plot(range(1, TRIALS+1), mlp_exp2, label='MLP', marker='x')
plt.axhline(g2, color='blue', linestyle='--', label='GNN Mean')
plt.axhline(m2, color='orange', linestyle='--', label='MLP Mean')
plt.xlabel('Trial')
plt.ylabel('Turnover / Trading Activity')
plt.title('EXP-2: Trading Activity vs Returns')
plt.legend()
plt.grid(True)
plt.show()

"""“Distilled MLP agents exhibit significantly stronger negative turnover–return correlations, indicating excessive, uncoordinated trading, while GNN-based inference mitigates overtrading through relational signals.”"""

# ---- EXP 3 ----
gnn_exp3 = [experiment_3_fat_tails(teacher, True) for _ in range(TRIALS)]
mlp_exp3 = [experiment_3_fat_tails(student, False) for _ in range(TRIALS)]

g3 = np.mean(gnn_exp3)
m3 = np.mean(mlp_exp3)

print("EXP-3 | Fat Tails (SF-I)")
print(f"GNN  : {g3:.4f}")
print(f"MLP  : {m3:.4f}")

# Plot EXP-3
plt.figure(figsize=(10,5))
plt.plot(range(1, TRIALS+1), gnn_exp3, label='GNN', marker='o')
plt.plot(range(1, TRIALS+1), mlp_exp3, label='MLP', marker='x')
plt.axhline(g3, color='blue', linestyle='--', label='GNN Mean')
plt.axhline(m3, color='orange', linestyle='--', label='MLP Mean')
plt.xlabel('Trial')
plt.ylabel('Fat Tails (SF-I)')
plt.title('EXP-3: Fat Tails')
plt.legend()
plt.grid(True)
plt.show()